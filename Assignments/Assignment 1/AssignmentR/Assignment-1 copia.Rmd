---
title: "Assignment 1"
author: "Álvaro y Pablo"
date: "10/6/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Step 1: Import the data-set

Cargamos las librerias a usar:

```{r, message=FALSE,warning=FALSE}
library(tidyverse)
library(MLTools)
library(randomForest)
library(caret)
library(GGally)
library(NeuralSens)
library(ROCR) 
library(corrplot)
library(rpart)
library(rpart.plot)
library(partykit)
library(NeuralNetTools) 
library(nnet)
library(ROSE)
```

Cargamos los datos:

```{r}
datos <- read.table("./data/Diabetes.csv", sep = ";", header = TRUE)
```

```{r include=FALSE}
datos <- datos %>%
  mutate(DIABETES = ifelse(DIABETES == 1, "Si", "No"))
datos <- datos %>%
  mutate(DIABETES = as.factor(DIABETES))
str(datos)
```


# Step 2: Check out the missing values

Resumen general del dataset:

```{r}
str(datos)
```

```{r}
summary(datos)
```

No tenemos ningún NA. 



# Step 3: Plot the data and check out for outliers.

Veamos a grandes rasgos una representación de las variables:

```{r message=FALSE, warning=FALSE}
ggpairs(datos, aes(color = DIABETES))
```

Da la sensación de existir bastantes valores atípicos, veamoslo variable a variable:

Comenzamos por la vaiable Glucose. Aplicaremos 3 técnicas diferentes para corregir los outliers y veremos cuál genera mejores resultados.
```{r}
boxplot_glucosa <- boxplot(datos$GLUCOSE)
boxplot_glucosa$out
```
Una persona con nivel de glucosa 0 es imposible que esté viva, por ello vamos a eliminar dichas observaciones.

```{r}
datos2 <- datos
datos2 <- datos2[datos2$GLUCOSE > 0, ]
```


```{r}
datos3 <- datos
datos3$GLUCOSE <- ifelse(datos3$GLUCOSE == 0, median(datos3$GLUCOSE), datos3$GLUCOSE)
```

```{r}
datos4 <- datos
datos4$GLUCOSE <- ifelse(datos4$GLUCOSE == 0, mean(datos4$GLUCOSE), datos4$GLUCOSE)
```


Comprobamos mirando un modelo sencillo cual es la mejor opción:


```{r}
trainIndex <- createDataPartition(datos2$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR2 <- datos2[trainIndex,]
fTS2 <- datos2[-trainIndex,]
fTR_eval2 <- fTR2
fTS_eval2 <- fTS2

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit2 <- train(form = DIABETES ~ GLUCOSE,
                    data = fTR2,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit2

```


```{r}

fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ GLUCOSE,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}

fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ GLUCOSE,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```

El modelo que mejor nos ha salido es en el que cambiamos los valores de 0 de la glucosa por la mediana.

Veamos ahora la presión sanguinea:
```{r}
boxplot_blood <- boxplot(datos$BLOODPRESS)
boxplot_blood$out

```
Volvemos a realizar lo mismo que en el caso anterior:
```{r}
datos2 <- datos2[datos2$BLOODPRESS > 0, ]
```


```{r}
datos3$BLOODPRESS <- ifelse(datos3$BLOODPRESS == 0, median(datos3$BLOODPRESS), datos3$BLOODPRESS)
```

```{r}
datos4$BLOODPRESS <- ifelse(datos4$BLOODPRESS == 0, mean(datos4$BLOODPRESS), datos4$BLOODPRESS)
```

```{r}
trainIndex <- createDataPartition(datos2$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR2 <- datos2[trainIndex,]
fTS2 <- datos2[-trainIndex,]
fTR_eval2 <- fTR2
fTS_eval2 <- fTS2

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit2 <- train(form = DIABETES ~ BLOODPRESS,
                    data = fTR2,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit2
```


```{r}

fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ BLOODPRESS,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}

fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ BLOODPRESS,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```

El modelo que mejor resultado proporciona es cambiar los valores 0 por la media.



Veamos ahora el bodymassindex:
```{r}
boxplot_bodymass <- boxplot(datos$BODYMASSINDEX)
boxplot_bodymass$out

```
Hacemos lo mismo con el bodymassindex:
```{r}
datos2 <- datos2[datos2$BODYMASSINDEX > 0, ]
```

```{r}
datos3$BODYMASSINDEX <- ifelse(datos3$BODYMASSINDEX == 0, median(datos3$BODYMASSINDEX), datos3$BODYMASSINDEX)
```

```{r}
datos4$BODYMASSINDEX <- ifelse(datos4$BODYMASSINDEX == 0, mean(datos4$BODYMASSINDEX), datos4$BODYMASSINDEX)
```

Y entrenamos modelos:

```{r}
trainIndex <- createDataPartition(datos2$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR2 <- datos2[trainIndex,]
fTS2 <- datos2[-trainIndex,]
fTR_eval2 <- fTR2
fTS_eval2 <- fTS2

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit2 <- train(form = DIABETES ~ BODYMASSINDEX,
                    data = fTR2,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit2
```


```{r}

fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ BODYMASSINDEX,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}

fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ BODYMASSINDEX,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```

El que mejor resultaods produce es cambiar los valores de 0 por la mediana.



Hacemos lo mismo con el insulina, lo unico que en este caso no contemplamos eliminar los datos que son 0 ya que representan gran cantidad de nuestro dataset.
```{r}
boxplot_insulin <- boxplot(datos$INSULIN)
boxplot_insulin$out
```

```{r}
datos3$INSULIN <- ifelse(datos3$INSULIN == 0, median(datos3$INSULIN), datos3$INSULIN)
```

```{r}
datos4$INSULIN <- ifelse(datos4$INSULIN == 0, mean(datos4$INSULIN), datos4$INSULIN)
```

Y entrenamos modelos:
```{r}
trainIndex <- createDataPartition(datos3$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ INSULIN,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}

fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ INSULIN,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```
Lo que mejor funciona es cambiar los 0 por la media



Por último nos queda ver la variable Sthickness que era la otra que tenía datos atípicos a simple vista:
```{r}
boxplot_sthick <- boxplot(datos$SKINTHICKNESS)
boxplot_sthick$out
```

Aunque no se vean tambien existen valores atípicos, que son erroneos, como es el caso del 0.

Por tanto nos encontramos en el mismo caso:
```{r}
datos2 <- datos2[datos2$SKINTHICKNESS > 0 & datos2$SKINTHICKNESS < 90, ]
```


```{r}
datos3$SKINTHICKNESS <- ifelse(datos3$SKINTHICKNESS == 0 , median(datos3$SKINTHICKNESS), datos3$SKINTHICKNESS)
datos3$SKINTHICKNESS <- ifelse(datos3$SKINTHICKNESS > 90 , median(datos3$SKINTHICKNESS), datos3$SKINTHICKNESS)
```

```{r}
datos4$SKINTHICKNESS <- ifelse(datos4$SKINTHICKNESS == 0, mean(datos4$SKINTHICKNESS), datos4$SKINTHICKNESS)
datos4$SKINTHICKNESS <- ifelse(datos4$SKINTHICKNESS > 90, mean(datos4$SKINTHICKNESS), datos4$SKINTHICKNESS)
```

Y entrenamos modelos:
```{r}
trainIndex <- createDataPartition(datos2$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR2 <- datos2[trainIndex,]
fTS2 <- datos2[-trainIndex,]
fTR_eval2 <- fTR2
fTS_eval2 <- fTS2

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit2 <- train(form = DIABETES ~ SKINTHICKNESS,
                    data = fTR2,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit2
```

```{r}

fTR3 <- datos3[trainIndex,]
fTS3 <- datos3[-trainIndex,]
fTR_eval3 <- fTR3
fTS_eval3 <- fTS3

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit3 <- train(form = DIABETES ~ SKINTHICKNESS,
                    data = fTR3,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit3
```


```{r}

fTR4 <- datos4[trainIndex,]
fTS4 <- datos4[-trainIndex,]
fTR_eval4 <- fTR4
fTS_eval4 <- fTS4

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
                     classProbs = TRUE) 


LogReg.fit4 <- train(form = DIABETES ~ SKINTHICKNESS,
                    data = fTR4,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
LogReg.fit4
```
Lo que mejor funciona es cambiar los outliers por la mediana.


Hacemos todos los cambios que hemos ido deciciendo sobre el dataset original:
```{r}
datos$GLUCOSE <- ifelse(datos$GLUCOSE == 0, median(datos$GLUCOSE), datos$GLUCOSE)
datos$BODYMASSINDEX <- ifelse(datos$BODYMASSINDEX == 0, mean(datos$BODYMASSINDEX), datos$BODYMASSINDEX)
datos$BLOODPRESS <- ifelse(datos$BLOODPRESS == 0, mean(datos$BLOODPRESS), datos$BLOODPRESS)
datos$BODYMASSINDEX <- ifelse(datos$BODYMASSINDEX == 0, median(datos$BODYMASSINDEX), datos$BODYMASSINDEX)
datos$INSULIN <- ifelse(datos$INSULIN == 0, mean(datos$INSULIN), datos$INSULIN)
datos$INSULIN <- ifelse(datos$INSULIN > 600, mean(datos$INSULIN), datos$INSULIN)
datos$SKINTHICKNESS <- ifelse(datos$SKINTHICKNESS == 0, median(datos$SKINTHICKNESS), datos$SKINTHICKNESS)
datos$SKINTHICKNESS <- ifelse(datos$SKINTHICKNESS > 90, median(datos$SKINTHICKNESS), datos$SKINTHICKNESS)
```


Y volvemos a hacer un EDA:

```{r}
ggpairs(datos, aes(color = DIABETES))
```
Todo bien, todo correcto.




# Step 4: Encode the categorical variables.

En el step anterior gracias a **str** hemos observado que nuestra variable de salida esta como un entero y nosotros la necesitamos como un factor. Como la variables toma solo el 0 y 1 y dichos valores no son correctos para los diferentes niveles dentro de un factor vamos a cambiar el nombre por "Si" y "No" y posteriormente lo pasamos a factor. 

A pesar de que deberia estar en esta parte dicho cambio lo hemos hecho al principio del documento por conveniencia grupal.
 



# Step 5 : Analyze the continuous variables.

```{r}
ggpairs(datos, aes(col = DIABETES))
```

Vemos que las escalas son muy diferentes, por ello tendremos que hacer una estandarización. Pero esta se implementará cuando nos pongamos a trabajar con los modelos.

# Step 6 : Check out for Class Imbalances.

Veamos como se reparten los valores en nuestra variable de salida:

```{r}
table(datos$DIABETES)
```

Es obvio que nuestra clase esta desbalanceada pero miremos un modelo sencillo como la regresión logistica para ver si nos mide bien la salida de Si.

```{r}
trainIndex_Bal <- createDataPartition(datos$DIABETES,     
                                  p = 0.8,      
                                  list = FALSE, 
                                  times = 1)
fTR_bal <- datos[trainIndex_Bal,]
fTS_bal <- datos[-trainIndex_Bal,]
fTR_eval_bal <- fTR_bal
fTS_eval_bal <- fTS_bal

ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
                     number = 10,                          #Number of folds
                     summaryFunction = defaultSummary,    
                     classProbs = TRUE) 


LogReg_bal.fit <- train(form = DIABETES ~ .,
                    data = fTR_bal,
                    method = "glm",
                    preProcess = c("center","scale"),
                    metric = "Accuracy",
                    trControl = ctrl)

fTR_eval_bal$LRprob <- predict(LogReg_bal.fit, type="prob", newdata = fTR_bal) # predict probabilities
fTR_eval_bal$LRpred <- predict(LogReg_bal.fit, type="raw", newdata = fTR_bal) # predict classes 
#test
fTS_eval_bal$LRprob <- predict(LogReg_bal.fit, type="prob", newdata = fTS_bal) # predict probabilities
fTS_eval_bal$LRpred <- predict(LogReg_bal.fit, type="raw", newdata = fTS_bal) # predict classes
```

```{r}
confusionMatrix(data = fTR_eval_bal$LRpred, #Predicted classes
                reference = fTR_eval_bal$DIABETES, #Real observations
                positive = "Si") #Class labeled as Positive

confusionMatrix(fTS_eval_bal$LRpred, 
                fTS_eval_bal$DIABETES, 
                positive = "Si")
```

La capacidad de modelar bien los datos de la clase más pequeña no es del todo buena, por lo que hemos decidido balancear los datos. Para ello usaremos la libreria de R **ROSE**.

```{r}
datos_bal <- ovun.sample(DIABETES ~ ., 
                             data = datos, 
                             method = "over", 
                             N = table(datos$DIABETES)[1]*2)$data
table(datos_bal$DIABETES)
```



# Step 7: Split the data-set into Training, Validation and Test Sets.

A continuación definiremos los parametros de control asi como tambien dividiremos el data-set en varios conjuntos.

```{r}
ctrl <- trainControl(method = "cv",number = 10, summaryFunction = defaultSummary, classProbs = TRUE) 
```

```{r}
trainIndex <- createDataPartition(datos_bal$DIABETES, p = 0.8, list = FALSE, times = 1)
```



# Identification and fitting process of classification models

Lo primero que hacemos antes de realizar ningún modelo será informarnos acerca del tema, con la finalidad de conocer cuales son las variables que más afectan a la diabetes.

Tras leer numerosos articulos podemos llegar a la conclusion que las variables que mas afectan a nuestra variable respuesta son tener obesidad, edad, presión arterial alta, antecedentes familiares y altos niveles de glucosa. 

Comenzamos con los diferentes modelos:

## Regresión Logística

Primero hacemos uno general para ver como se comportan las variables:

```{r}
fTR1 <- datos_bal[trainIndex,] 
fTS1 <- datos_bal[-trainIndex,]

fTR1_eval <- fTR1
fTS1_eval <- fTS1

LogReg.fit <- train(form = DIABETES ~  ., data = fTR1, method = "glm",
            preProcess = c("center","scale"), trControl = ctrl,metric = "Accuracy")    

LogReg.fit
```


```{r}
summary(LogReg.fit)
```

```{r}
fTR1_eval$LRprob <- predict(LogReg.fit, type="prob", newdata = fTR1) 
fTR1_eval$LRpred <- predict(LogReg.fit, type="raw", newdata = fTR1) 

fTS1_eval$LRprob <- predict(LogReg.fit, type="prob", newdata = fTS1) 
fTS1_eval$LRpred <- predict(LogReg.fit, type="raw", newdata = fTS1) 
```


```{r}
Plot2DClass(fTR1[,], 
            fTR1$DIABETES,    
            LogReg.fit,
            var1 = "GLUCOSE", var2 = "BLOODPRESS", 
            selClass = "Si")     
```

```{r}
confusionMatrix(data = fTR1_eval$LRpred, 
                reference = fTR1_eval$DIABETES, 
                positive = "Si") 
```

```{r}
confusionMatrix(fTS1_eval$LRpred, 
                fTS1_eval$DIABETES, 
                positive = "Si")
```

```{r}
PlotClassPerformance(fTR1_eval$DIABETES,       #Real observations
                     fTR1_eval$LRprob,  #predicted probabilities
                     selClass = "Si") #Class to be analyzed
```

```{r}
PlotClassPerformance(fTS1_eval$DIABETES,       #Real observations
                     fTS1_eval$LRprob,  #predicted probabilities
                     selClass = "Si") #Class to be analyzed)
```
Tras este primer modelo lo que podemos hacer es un segundo modelo pero solo teniendo en cuenta a las variables importantes:

```{r}
fTR1_2 <- datos_bal[trainIndex,] 
fTS1_2 <- datos_bal[-trainIndex,]

fTR1_2_eval <- fTR1_2
fTS1_2_eval <- fTS1_2

LogReg2.fit <- train(form = DIABETES ~ PREGNANT + BODYMASSINDEX + GLUCOSE + PEDIGREEFUNC , data = fTR1_2, 
              method = "glm", preProcess = c("center","scale"), trControl = ctrl,metric = "Accuracy")    

LogReg2.fit
```

```{r}
summary(LogReg2.fit)
```

```{r}
fTR1_2_eval$LRprob <- predict(LogReg2.fit, type="prob", newdata = fTR1_2) 
fTR1_2_eval$LRpred <- predict(LogReg2.fit, type="raw", newdata = fTR1_2) 

fTS1_2_eval$LRprob <- predict(LogReg2.fit, type="prob", newdata = fTS1_2) 
fTS1_2_eval$LRpred <- predict(LogReg2.fit, type="raw", newdata = fTS1_2) 
```


```{r}
Plot2DClass(fTR1_2[,c(1,2,6,7)], 
            fTR1_2$DIABETES,    
            LogReg2.fit,
            var1 = "GLUCOSE", var2 = "BODYMASSINDEX", 
            selClass = "Si")     
```

```{r}
confusionMatrix(data = fTR1_2_eval$LRpred, 
                reference = fTR1_2_eval$DIABETES, 
                positive = "Si") 
```


```{r}
confusionMatrix(data = fTS1_2_eval$LRpred, 
                reference = fTS1_2_eval$DIABETES, 
                positive = "Si") 
```


```{r}
PlotClassPerformance(fTR1_2_eval$DIABETES,       
                     fTR1_2_eval$LRprob,  
                     selClass = "Si") 
```

```{r}
PlotClassPerformance(fTS1_2_eval$DIABETES,      
                     fTS1_2_eval$LRprob,  
                     selClass = "Si") 
```
## Knn

```{r}
fTR2 <- datos_bal[trainIndex,] 
fTS2 <- datos_bal[-trainIndex,]

fTR2_eval <- fTR2
fTS2_eval <- fTS2

knn.fit = train(form = DIABETES ~ .,
                data = fTR2,   
                method = "knn",
                preProcess = c("center","scale"),
                tuneGrid = data.frame(k = seq(2,40,1)),
                trControl = ctrl, 
                metric = "Accuracy")
knn.fit
```


```{r}
ggplot(knn.fit)
```

```{r}
knn.fit$finalModel
```

```{r}
fTR2_eval$knn_prob <- predict(knn.fit, type="prob" , newdata = fTR2)
fTR2_eval$knn_pred <- predict(knn.fit, type="raw" , newdata = fTR2)

fTS2_eval$knn_prob <- predict(knn.fit, type="prob" , newdata = fTS2) 
fTS2_eval$knn_pred <- predict(knn.fit, type="raw" , newdata = fTS2)
```


```{r}
Plot2DClass(fTR2[,],
            fTR2$DIABETES,    
            knn.fit,
            var1 = "GLUCOSE", var2 = "BODYMASSINDEX", 
            selClass = "Si")     
```

```{r}
confusionMatrix(data = fTR2_eval$knn_pred,
                reference = fTR2_eval$DIABETES, 
                positive = "Si")
```


```{r}
confusionMatrix(fTS2_eval$knn_pred, 
                fTS2_eval$DIABETES, 
                positive = "Si")
```


```{r}
PlotClassPerformance(fTR2_eval$DIABETES,       
                     fTR2_eval$knn_prob,  
                     selClass = "Si") 
```

```{r}
PlotClassPerformance(fTS2_eval$DIABETES,       
                     fTS2_eval$knn_prob,  
                     selClass = "Si") 
```

## Arboles de decisión

```{r}
fTR3 <- datos_bal[trainIndex,]
fTS3 <- datos_bal[-trainIndex,]

fTR3_eval <- fTR3
fTS3_eval <- fTS3

inputs <- 1:(ncol(datos_bal)-1)
tree.fit <- train(x = fTR3[,inputs],  
                 y = fTR3$DIABETES,  
                 method = "rpart", 
                 control = rpart.control(minsplit = 10,  
                                        minbucket = 10),
                 parms = list(split = "gini"),
                 tuneGrid = data.frame(cp = seq(0,0.1,0.001)),
                 trControl = ctrl, 
                 metric = "Accuracy")
tree.fit
```

```{r}
ggplot(tree.fit)
```

```{r}
tree.fit$finalModel
```


```{r}
rpart.plot(tree.fit$finalModel, type = 2, fallen.leaves = FALSE, box.palette = "Oranges")
```

```{r}
varImp(tree.fit,scale = FALSE)
plot(varImp(tree.fit,scale = FALSE))
```


```{r}
fTR3_eval$tree_prob <- predict(tree.fit, type="prob", newdata = fTR3) 
fTR3_eval$tree_pred <- predict(tree.fit, type="raw", newdata = fTR3)

fTS3_eval$tree_prob <- predict(tree.fit, type="prob", newdata = fTS3) 
fTS3_eval$tree_pred <- predict(tree.fit, type="raw", newdata = fTS3)
```


```{r}
Plot2DClass(fTR3[,],
            fTR3$DIABETES,    
            tree.fit,
            var1 = "GLUCOSE", var2 = "BODYMASSINDEX", 
            selClass = "Si")
```


```{r}
confusionMatrix(data = fTR3_eval$tree_pred, 
                reference = fTR3_eval$DIABETES, 
                positive = "Si")
```

```{r}
confusionMatrix(fTS3_eval$tree_pred, 
                fTS3_eval$DIABETES, 
                positive = "Si")
```

```{r}
PlotClassPerformance(fTR3_eval$DIABETES,     
                     fTR3_eval$tree_prob,  
                     selClass = "Si") 
```

```{R}
PlotClassPerformance(fTS3_eval$DIABETES,       
                     fTS3_eval$tree_prob,  
                     selClass = "Si")
```

Ya hemos visto las variables importantes de nuestro modelo. Hagamos a continuación otro modelo con unicamente las variables importantes.

```{r}
fTR3_2 <- datos_bal[trainIndex,]
fTS3_2 <- datos_bal[-trainIndex,]

fTR3_2_eval <- fTR3_2
fTS3_2_eval <- fTS3_2

inputs <- c(1,2,5,6,7,8)
tree2.fit <- train(x = fTR3_2[,inputs],  
                 y = fTR3_2$DIABETES,  
                 method = "rpart", 
                 control = rpart.control(minsplit = 10,  
                                        minbucket = 15),
                 parms = list(split = "gini"),
                 tuneGrid = data.frame(cp = seq(0,0.1,0.001)),
                 trControl = ctrl, 
                 metric = "Accuracy")
tree2.fit
```

```{r}
ggplot(tree2.fit)
```

```{r}
rpart.plot(tree2.fit$finalModel, type = 2, fallen.leaves = FALSE, box.palette = "Oranges")
```

```{r}
fTR3_2_eval$tree_prob <- predict(tree2.fit, type="prob", newdata = fTR3_2) 
fTR3_2_eval$tree_pred <- predict(tree2.fit, type="raw", newdata = fTR3_2)

fTS3_2_eval$tree_prob <- predict(tree2.fit, type="prob", newdata = fTS3_2) 
fTS3_2_eval$tree_pred <- predict(tree2.fit, type="raw", newdata = fTS3_2)
```


```{r}
Plot2DClass(fTR3_2[,],
            fTR3_2$DIABETES,    
            tree2.fit,
            var1 = "GLUCOSE", var2 = "BODYMASSINDEX", 
            selClass = "Si")
```

```{r}
confusionMatrix(data = fTR3_2_eval$tree_pred, 
                reference = fTR3_2_eval$DIABETES, 
                positive = "Si")
```

```{r}
confusionMatrix(data = fTS3_2_eval$tree_pred, 
                reference = fTS3_2_eval$DIABETES, 
                positive = "Si")
```

```{r}
PlotClassPerformance(fTR3_2_eval$DIABETES,     
                     fTR3_2_eval$tree_prob,  
                     selClass = "Si") 
```


```{r}
PlotClassPerformance(fTS3_2_eval$DIABETES,     
                     fTS3_2_eval$tree_prob,  
                     selClass = "Si") 
```






## SVM

```{r}
fTR4 <- datos_bal[trainIndex,]
fTS4 <- datos_bal[-trainIndex,]

fTR4_eval <- fTR4
fTS4_eval <- fTS4

svm.fit <- train(form = DIABETES ~ ., 
                data = fTR4,   
                method = "svmLinear", 
                preProcess = c("center","scale"),
                tuneGrid = data.frame(C = seq(0.1,10,0.2)),
                trControl = ctrl, 
                metric = "Accuracy")
svm.fit
```

```{r}
ggplot(svm.fit) + scale_x_log10()
```

```{r}
svm.fit$finalModel
```

```{r}
fTR4_eval$svm_prob <- predict(svm.fit, type="prob", newdata = fTR4) 
fTR4_eval$svm_pred <- predict(svm.fit, type="raw", newdata = fTR4) 

fTS4_eval$svm_prob <- predict(svm.fit, type="prob", newdata = fTS4) 
fTS4_eval$svm_pred <- predict(svm.fit, type="raw", newdata = fTS4)  
```

```{r}
Plot2DClass(fTR4[,], 
            fTR4$DIABETES,     
            svm.fit,
            var1 = "GLUCOSE", var2 = "BODYMASSINDEX", 
            selClass = "Si")     
```

```{r}
confusionMatrix(data = fTR4_eval$svm_pred, 
                reference = fTR4_eval$DIABETES, 
                positive = "Si") 
```

```{r}
confusionMatrix(fTS4_eval$svm_pred, 
                fTS4_eval$DIABETES, 
                positive = "Si")
```
```{r}
PlotClassPerformance(fTS4_eval$DIABETES,     
                     fTS4_eval$svm_prob,  
                     selClass = "Si") 
```


Ahora veamos un svm con metodo svmcircular.

```{r}
fTR4_2 <- datos_bal[trainIndex,]
fTS4_2 <- datos_bal[-trainIndex,]

fTR4_2_eval <- fTR4_2
fTS4_2_eval <- fTS4_2

svm2.fit <- train(form = DIABETES ~ ., 
                data = fTR4_2,   
                method = "svmRadial", 
                preProcess = c("center","scale"),
                tuneGrid = expand.grid(C = seq(0.1,100,length.out = 8), 
                                       sigma=seq(180.1,1000.1,length.out = 4)),            
                trControl = ctrl, 
                metric = "Accuracy")
svm2.fit
```


```{r}
ggplot(svm2.fit)
```

```{r}
svm2.fit$finalModel
```

```{r}
fTR4_2_eval$svm_prob <- predict(svm2.fit, type="prob", newdata = fTR4_2) 
fTR4_2_eval$svm_pred <- predict(svm2.fit, type="raw", newdata = fTR4_2) 

fTS4_2_eval$svm_prob <- predict(svm2.fit, type="prob", newdata = fTS4_2) 
fTS4_2_eval$svm_pred <- predict(svm2.fit, type="raw", newdata = fTS4_2)  
```

```{r}
Plot2DClass(fTR4_2[,], 
            fTR4_2$DIABETES,     
            svm2.fit,
            var1 = "GLUCOSE", var2 = "BODYMASSINDEX", 
            selClass = "Si")     
```

```{r}
Plot2DClass(fTS4_2[,], 
            fTS4_2$DIABETES,     
            svm2.fit,
            var1 = "GLUCOSE", var2 = "BODYMASSINDEX", 
            selClass = "Si")     
```





```{r}
confusionMatrix(fTR4_2_eval$svm_pred, 
                fTR4_2_eval$DIABETES, 
                positive = "Si")
```



```{r}
confusionMatrix(fTS4_2_eval$svm_pred, 
                fTS4_2_eval$DIABETES, 
                positive = "Si")
```

```{r}
PlotClassPerformance(fTS4_2_eval$DIABETES,     
                     fTS4_2_eval$svm_prob,  
                     selClass = "Si") 
```






## Redes Neuronales

```{r}
fTR5 <- datos_bal[trainIndex,]
fTS5 <- datos_bal[-trainIndex,]

fTR5_eval = fTR5
fTS5_eval = fTS5

mlp.fit = train(form = DIABETES ~ .,
                data = fTR5,  
                method = "nnet",
                preProcess = c("center","scale"),
                maxit = 250,   
                
                tuneGrid = expand.grid(size = seq(5,15,length.out = 11),
                                       decay=c(10^(-9),0.0001,0.001,0.01,0.1,1)), 
                trControl = ctrl, 
                metric = "Accuracy")

mlp.fit
```


```{r}
ggplot(mlp.fit)+scale_x_log10()
```


```{r}
mlp.fit$finalModel
plotnet(mlp.fit$finalModel)
```


```{r}
SensAnalysisMLP(mlp.fit) 
```


```{r}
fTR5_eval$mlp_prob = predict(mlp.fit, type="prob" , newdata = fTR5)
fTR5_eval$mlp_pred = predict(mlp.fit, type="raw" , newdata = fTR5) 

fTS5_eval$mlp_prob = predict(mlp.fit, type="prob" , newdata = fTS5) 
fTS5_eval$mlp_pred = predict(mlp.fit, type="raw" , newdata = fTS5) 
```


```{r}
Plot2DClass(fTR5[,],
            fTR5$DIABETES,     
            mlp.fit,
            var1="GLUCOSE", var2="BODYMASSINDEX",
            selClass = "Si")  
```


```{r}
confusionMatrix(data = fTR5_eval$mlp_pred, 
                reference = fTR5_eval$DIABETES, 
                positive = "Si") 
```


```{r}
confusionMatrix(fTS5_eval$mlp_pred, 
                fTS5_eval$DIABETES, 
                positive = "Si")
```


```{r}
PlotClassPerformance(fTR5_eval$DIABETES,
                     fTR5_eval$mlp_prob, 
                     selClass = "Si") 
```

```{r}
PlotClassPerformance(fTS5_eval$DIABETES,      
                     fTS5_eval$mlp_prob, 
                     selClass = "Si")
```



Hacemos otro con las variables más importantes:


```{r}
fTR5_2 <- datos_bal[trainIndex,]
fTS5_2 <- datos_bal[-trainIndex,]

fTR5_2_eval = fTR5_2
fTS5_2_eval = fTS5_2

mlp2.fit = train(form = DIABETES ~ (BODYMASSINDEX)^2 + BODYMASSINDEX + AGE + SKINTHICKNESS + GLUCOSE + BLOODPRESS ,
                data = fTR5_2,  
                method = "nnet",
                preProcess = c("center","scale"),
                maxit = 250,   
                
                tuneGrid = expand.grid(size = seq(5,25,length.out = 11),
                                       decay=c(10^(-9),0.0001,0.001,0.01)), 
                trControl = ctrl, 
                metric = "Accuracy")

mlp2.fit
```

```{r}
ggplot(mlp2.fit)+scale_x_log10()
```

```{r}
fTR5_2_eval$mlp_prob = predict(mlp2.fit, type="prob" , newdata = fTR5_2)
fTR5_2_eval$mlp_pred = predict(mlp2.fit, type="raw" , newdata = fTR5_2) 

fTS5_2_eval$mlp_prob = predict(mlp2.fit, type="prob" , newdata = fTS5_2) 
fTS5_2_eval$mlp_pred = predict(mlp2.fit, type="raw" , newdata = fTS5_2) 
```


```{r}
Plot2DClass(fTR5_2[,],
            fTR5_2$DIABETES,     
            mlp2.fit,
            var1="GLUCOSE", var2="BODYMASSINDEX",
            selClass = "Si")  
```

```{r}
confusionMatrix(data = fTR5_2_eval$mlp_pred, 
                reference = fTR5_2_eval$DIABETES, 
                positive = "Si") 
```


```{r}
confusionMatrix(data = fTS5_2_eval$mlp_pred, 
                reference = fTS5_2_eval$DIABETES, 
                positive = "Si") 
```

```{r}
PlotClassPerformance(fTR5_2_eval$DIABETES,
                     fTR5_2_eval$mlp_prob, 
                     selClass = "Si") 
```



```{r}
PlotClassPerformance(fTS5_2_eval$DIABETES,
                     fTS5_2_eval$mlp_prob, 
                     selClass = "Si") 
```



## Random Forest

```{r}
fTR6 <- datos_bal[trainIndex,]
fTS6 <- datos_bal[-trainIndex,]

fTR6_eval = fTR6
fTS6_eval = fTS6

rf.fit = train(form = DIABETES ~ .,
                data = fTR6,  
                method = "rf",
                preProcess = c("center","scale"),
                tunegrid = expand.grid(.mtry = (1:15)) , 
                trControl = ctrl, 
                metric = "Accuracy")

rf.fit
```

```{r}
rf.fit$finalModel
```

```{r}
fTR6_eval$rf_prob = predict(rf.fit, type="prob" , newdata = fTR6)
fTR6_eval$rf_pred = predict(rf.fit, type="raw" , newdata = fTR6) 

fTS6_eval$rf_prob = predict(rf.fit, type="prob" , newdata = fTS6) 
fTS6_eval$rf_pred = predict(rf.fit, type="raw" , newdata = fTS6) 
```


```{r}
confusionMatrix(data = fTR6_eval$rf_pred, 
                reference = fTR6_eval$DIABETES, 
                positive = "Si") 
```


```{r}
confusionMatrix(data = fTS6_eval$rf_pred, 
                reference = fTS6_eval$DIABETES, 
                positive = "Si") 
```

## Comparativa de modelos

```{r}
transformResults <- resamples(list(lr = LogReg.fit, lr2 = LogReg2.fit, knn = knn.fit, tree = tree.fit, tree2 = tree2.fit, svm = svm.fit, svm2 = svm2.fit, mlp = mlp.fit, mlp2 = mlp2.fit, rf = rf.fit ))
summary(transformResults)
dotplot(transformResults)
```



```{r}
#Testaccuracy
LR1 <- confusionMatrix(fTS1_eval$LRpred, fTS1_eval$DIABETES, positive = "Si")$overall[1]
LR2 <- confusionMatrix(fTS1_2_eval$LRpred, fTS1_2_eval$DIABETES, positive = "Si")$overall[1]
KNN <- confusionMatrix(fTS2_eval$knn_pred, fTS2_eval$DIABETES, positive = "Si")$overall[1]
TREE1 <- confusionMatrix(fTS3_eval$tree_pred, fTS3_eval$DIABETES, positive = "Si")$overall[1]
TREE2 <- confusionMatrix(fTS3_2_eval$tree_pred, fTS3_2_eval$DIABETES, positive = "Si")$overall[1]
SVM1 <- confusionMatrix(fTS4_eval$svm_pred, fTS4_eval$DIABETES, positive = "Si")$overall[1]
SVM2 <- confusionMatrix(fTS4_2_eval$svm_pred, fTS4_2_eval$DIABETES, positive = "Si")$overall[1]
MLP1 <- confusionMatrix(fTS5_eval$mlp_pred, fTS5_eval$DIABETES, positive = "Si")$overall[1]
MLP2 <- confusionMatrix(fTS5_2_eval$mlp_pred, fTS5_2_eval$DIABETES, positive = "Si")$overall[1]
RF <- confusionMatrix(fTS6_eval$rf_pred, fTS6_eval$DIABETES, positive = "Si")$overall[1]
dotchart(c(LR1, LR2, KNN, TREE1, TREE2, SVM1, SVM2, MLP1, MLP2, RF), c("LogReg1", "LogReg2", "Knn", "Tree Decision 1", "Tree Decision 2", "Vector Linear", "Vector Radial", "Neural Network 1", "Neural Network 2", "Random Forest"), bg = "black", main = "Test Accuracy")
```








## Extra

Hemos pensado en tratar la variable Pregnant como una variable discreta en lugar de continua. Tras ello evaluaremos algunos modelos para ver si ha mejorado el resultado.
```{r}
datos_bal$PREGNANT <- cut(datos_bal$PREGNANT, breaks = c(-1, 1, 2, 3, 4, 5, 99), labels = c("0", "1", "2", "3", "4", "+5"))
str(datos_bal)
```


Después de esto hacemos un modelo de Redes neuronales para ver si hemos conseguido una mejoría

```{r}
fTR6 <- datos_bal[trainIndex,]
fTS6 <- datos_bal[-trainIndex,]

fTR6_eval = fTR5
fTS6_eval = fTS5

mlp.fit = train(form = DIABETES ~ .,
                data = fTR6,  
                method = "nnet",
                preProcess = c("center","scale"),
                maxit = 250,   
                
                tuneGrid = expand.grid(size = seq(5,15,length.out = 11),
                                       decay=c(10^(-9),0.0001,0.001,0.01,0.1,1)), 
                trControl = ctrl, 
                metric = "Accuracy")

mlp.fit
```


```{r}
ggplot(mlp.fit)+scale_x_log10()
```


```{r}
mlp.fit$finalModel
plotnet(mlp.fit$finalModel)
```


```{r}
SensAnalysisMLP(mlp.fit) 
```


```{r}
fTR6_eval$mlp_prob = predict(mlp.fit, type="prob" , newdata = fTR6)
fTR6_eval$mlp_pred = predict(mlp.fit, type="raw" , newdata = fTR6) 

fTS6_eval$mlp_prob = predict(mlp.fit, type="prob" , newdata = fTS6) 
fTS6_eval$mlp_pred = predict(mlp.fit, type="raw" , newdata = fTS6) 
```


```{r}
Plot2DClass(fTR6[,],
            fTR6$DIABETES,     
            mlp.fit,
            var1="GLUCOSE", var2="BODYMASSINDEX",
            selClass = "Si")  
```


```{r}
confusionMatrix(data = fTR6_eval$mlp_pred, 
                reference = fTR6_eval$DIABETES, 
                positive = "Si") 
```


```{r}
confusionMatrix(fTS6_eval$mlp_pred, 
                fTS6_eval$DIABETES, 
                positive = "Si")
```


```{r}
PlotClassPerformance(fTR6_eval$DIABETES,
                     fTR6_eval$mlp_prob, 
                     selClass = "Si") 
```

```{r}
PlotClassPerformance(fTS6_eval$DIABETES,      
                     fTS6_eval$mlp_prob, 
                     selClass = "Si")
```

Ahora comparamos este nuevo modelo de Redes Neuronales con los 2 anteriores que habíamos creado.
```{r}
MLP1 <- confusionMatrix(fTS5_eval$mlp_pred, fTS5_eval$DIABETES, positive = "Si")$overall[1]
MLP2 <- confusionMatrix(fTS5_2_eval$mlp_pred, fTS5_2_eval$DIABETES, positive = "Si")$overall[1]
MLP3 <- confusionMatrix(fTS6_eval$mlp_pred, fTS6_eval$DIABETES, positive = "Si")$overall[1]

dotchart(c(MLP1, MLP2, MLP3), c("Neural Network 1", "Neural Network 2", "NN3"), bg = "black", main = "Test Accuracy")
```
Llegamos a la conclusión que este nuevo modelo funciona un poco mejor que los 2 anteriores.


